[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Overview\nThis collection of workshops provides an introduction to machine learning. The collection has two standalone parts:\n\nOverview of Machine Learning (one 2-hour session): This is a non-technical workshop that emphasizes building vocabulary and gaining an intuitive understanding of machine learning concepts and methods. Start here if you‚Äôre new to machine learning and want to get a sense of what it‚Äôs about and whether it‚Äôs relevant to you. There‚Äôs no code in this workshop and only a little (high-school level) math. This workshop is also good preparation for the Machine Learning in R series.\n\n\n\n\n\n\nLearning Goals\n\n\n\n\n\nAfter completing this workshop, learners should be able to:\n\nDefine the following terms: observation, feature, machine learning, supervised learning, unsupervised learning, regression, classification, clustering, training set, validation set, test set, cross-validation, overfitting, underfitting, model bias, model variance, bias-variance tradeoff, ensemble model.\nExplain the difference between supervised and unsupervised learning.\nExplain the difference between regression and classification.\nList and briefly describe popular machine learning methods.\nGive an example of an ensemble model.\nExplain what cross-validation is used for and give an overview of the procedure.\nAssess whether and which machine learning methods might be helpful for a given research problem.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis slide deck is the only material for this workshop.\n\n\nMachine Learning in R (two 2-hour sessions): This is a hands-on, technical introduction to using machine learning methods in R. The two sessions cover and include examples of supervised learning (emphasis on classification), model evaluation, unsupervised learning (emphasis on clustering), and dimension reduction. The sessions also provide advice for navigating R‚Äôs fractured machine learning package landscape. Intermediate familiarity with R programming (equivalent to completing DataLab‚Äôs R Basics workshop series) is required.\n\n\n\n\n\n\nLearning Goals\n\n\n\n\n\nAfter completing this series, learners should be able to:\n\nBuild and train a classification model on their data.\nUse cross-validation to estimate accuracy and tune hyperparameters for classification models.\nIdentify strategies to improve results from classification models.\nExplain the tradeoffs between popular clustering algorithms.\nRun a clustering algorithm on their data.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html",
    "href": "chapters/01_supervised.html",
    "title": "1¬† Supervised Learning",
    "section": "",
    "text": "1.1 Introduction\nSupervised learning methods learn from training data in order to make predictions about new data.\nBefore going further, let‚Äôs review some necessary vocabulary:\nFor example, suppose you‚Äôve collected a data set with the age, height, and species of every tree in your neighborhood. Each tree is a unit. Each (age, height, species) measurement for a tree is an observation. The features are age, height, and species. If you want to use supervised learning to predict height based on age and species, then height is a response. üå≤üå≥üå¥\nProblems and methods are conventionally described in terms of the type of response:\nContinuing the trees example, if you want to predict height in meters, you have a regression problem. If you want to predict species, you have a classification problem.\nThis chapter focuses on classification, but many of the concepts covered here are equally relevant to regression. Moreover, there are regression variants of many classification methods. In fact, many classifiers are just a combination of a regression method that predicts class probabilities and a decision rule that selects a class based on the prediction.\nA supervised learning method consists of:\nMethods ‚Äúlearn‚Äù by selecting parameter values that minimize loss for predictions about training data, in the hope that these values will generalize well to new data.\nMost models also have hyperparameters (or tuning parameters), parameters that are not learned and must instead be specified before training. Usually, these control how much the model adapts to the training data. To select hyperparameter values, use your own knowledge about the data as well as model evaluation techniques (see Section 1.5).\nThe next section presents an overview of packages for supervised learning in R, some of which are demonstrated in later examples. Subsequent sections explain how to select features, how to select a model, and common model evaluation strategies, including data partitioning and cross-validation. The chapter ends with a short discussion of how to improve model performance and where to go to learn more.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#introduction",
    "href": "chapters/01_supervised.html#introduction",
    "title": "1¬† Supervised Learning",
    "section": "",
    "text": "A unit is an entity of interest in a data set. Units are sometimes also called subjects.\nAn observation is all measurements for a single unit. Observations are sometimes also called cases.\nA feature is a single measurement across all units. Features are sometimes also called variables, covariates, or predictors.\nA response is a feature that you want to predict.\n\n\n\n\nRegression refers to a numerical response.\nClassification refers to a categorical response. We call the categories classes.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to learn more about regression, see DataLab‚Äôs Introduction to Regression Modeling in R workshop series.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nClassification methods are reductive: they assume that classes are mutually exclusive and only predict one class for each data point.\nIn contexts where predictions are meant to inform people making decisions, regression methods are often more appropriate and yield more informative predictions (for example, class probabilities). Frank Harrell (Professor of Biostatistics, Vanderbilt University) discusses this in his post Classification vs.¬†Prediction.\nOn the other hand, classification methods often work well for automating decisions or tasks that are straightforward but tedious, especially if the stakes are low.\n\n\n\n\nA model for making predictions, with assumptions and adjustable parameters that govern what predictions are made.\nA loss function for measuring the real or abstract cost of incorrect predictions.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#packages",
    "href": "chapters/01_supervised.html#packages",
    "title": "1¬† Supervised Learning",
    "section": "1.2 Packages",
    "text": "1.2 Packages\nThe R community has developed dozens of machine learning packages to supplement R‚Äôs small selection of built-in functions. A majority of these provide a single method or family of methods (such as decision trees).\nThe CRAN Task View: Machine Learning page is a good starting point for finding packages related to supervised learning. The task view is regularly updated by the CRAN administrators and only lists popular, actively-maintained packages.\n\n\n\n\n\n\n\nTip\n\n\n\nIf the task view doesn‚Äôt list a package for a method you want to use, try searching the web. Many packages hosted on CRAN or GitHub are not listed in the task view because they‚Äôre obscure, unmaintained, or in early development. None of these things mean a package won‚Äôt work, although you might want to check the package code carefully to make sure it does what you expect.\n\n\nDifferent packages generally provide different programming interfaces. This makes studying machine learning in R difficult: in addition to learning each method, you must also learn the idiosyncrasies of each package. This also makes it difficult to compare methods by swapping them in and out of a data processing pipeline. To address this, some community members have developed packages that provide a common programming interface on top of existing, independent methods packages. Others have developed omnibus packages that implement a wide variety of methods. These packages are listed below from most to least CRAN downloads in March 2024.\n\ncaret is a common interface for training models with existing packages, as well as a collection of functions for related tasks (such as splitting data).\ntidymodels is a metapackage (a collection of packages) for supervised learning designed to work well with tidyverse packages. In some respects, this is the successor to caret. Especially important are the packages:\n\nparsnip provides a common interface for existing methods packages, or ‚Äúengines‚Äù in the terminology of parsnip. For some methods, multiple engines are available, while for others, there is only one.\nrsample provides functions to partition and sample data sets.\nyardstick provides functions to compute a variety of performance statistics.\nbroom provides functions to summarize model results in data frames.\n\nh2o is an omnibus machine learning package, with implementations of many popular supervised and unsupervised learning methods. h2o is available for many different programming languages and places emphasis on computational efficiency.\nmlr3 and mlr3learners provide a common, object-oriented interface for existing methods packages.\nSuperLearner is an omnibus machine learning package designed to evaluate multiple methods and select the best combination of them.\nqeML is a relatively new package that provides a common interface for existing methods, intended to be easier to use than tidymodels and mlr3. qeML is developed by UC Davis Professor Emeritus Norm Matloff.\n\nThe caret, tidymodels, and h2o packages are all relatively complete solutions with detailed documentation, many users, and many contributors.\n\n1.2.1 Example: Classifying Penguins\n\n\n\nChinstrap, Gentoo, and Ad√©lie penguins. Artwork by Allison Horst.\n\n\nIn this example, we‚Äôll fit a \\(k\\)-nearest neighbors (kNN) classifier with the tidymodels package. In \\(k\\)-nearest neighbors, new observations are classified by taking a majority vote of the classes of the \\(k\\) nearest training set observations. Generally, we choose odd \\(k\\) to prevent ties, although there are schemes for breaking ties if \\(k\\) is even.\nAs an example data set, we‚Äôll use the Palmer Penguins data set, which was collected by Dr.¬†Kristen Gorman at Palmer Station, Antarctica. The data set records physical characteristics for hundreds of individual penguins from three different species: Ad√©lie, Chinstrap, and Gentoo.\nThe data set is available for R in the palmerpenguins package. We‚Äôll also use several other packages in this example. To install them all:\n\ninstall.packages(\"palmerpenguins\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"tidymodels\")\n\nLoad the palmerpenguins package, which will automatically create a penguins variable, and take a look at the first few observations:\n\nlibrary(\"palmerpenguins\")\n\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIt‚Äôs a good idea to do some exploratory analysis before fitting a model. We can use ggplot2 to plot bill length, body mass, and species for each penguin:\n\nlibrary(\"ggplot2\")\n\nplt = ggplot(penguins) + geom_point() + aes(shape = species, color = species)\n\nplt %+% aes(x = bill_length_mm, y = body_mass_g)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Other plots that might be of interest:\n# plt %+% aes(x = bill_length_mm, y = bill_depth_mm)\n# plt %+% aes(x = bill_length_mm, y = flipper_length_mm)\n# plt %+% aes(x = bill_depth_mm, y = body_mass_g)\n# plt %+% aes(x = bill_depth_mm, y = flipper_length_mm)\n# plt %+% aes(x = flipper_length_mm, y = body_mass_g)\n\nThe plot shows that the three species of penguins are relatively well-separated by bill length and body mass. Ad√©lie penguins have relatively short bills and light bodies, Chinstrap penguins have relatively long bills and light bodies, and Gentoo penguins have relatively long bills and heavy bodies. The species only overlap for a few penguins. This separation is a sign that a classifier is likely to work well.\n\n\n\n\n\n\nTip\n\n\n\nClassification is more difficult when the classes are not well-separated by the features, but not necessarily intractible. Sometimes it‚Äôs possible to transform features to provide better separation. Some models can also take advantage of interactions between features that are not readily apparent in scatter plots.\n\n\nInitial exploratory analysis is also when you should check data for outliers and missing values. The previous plot didn‚Äôt show any signs of extreme outliers. Now let‚Äôs check for missing values:\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nkNN classifiers can‚Äôt handle missing values, and only a few observation have them, so let‚Äôs remove these observations from the data set:\n\npeng = na.omit(penguins)\n\nNow let‚Äôs train a kNN classifier with tidymodels. Specifically, we‚Äôll use the parsnip package, which is included with tidymodels and provides a common interface for a wide variety of supervised learning models.\nTraining with parsnip follows the same two steps for every model:\n\nlibrary(\"parsnip\")\n\n1knn = nearest_neighbor(\"classification\", neighbors = 10)\n2fitted = fit(knn, species ~ bill_length_mm + body_mass_g, peng)\n\n\n1\n\nInitialize a model and set hyperparameters by calling a model function. In this case, we use nearest_neighbor since we want a kNN model, and set the number of neighbors to 10.\n\n2\n\nTrain the model by calling fit with the model, a formula ~ that specifies the response on the left-hand side and the predictors on the right-hand side, and the data.\n\n\n\n\nYou can print the fitted model to get more information:\n\nfitted\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = species ~ bill_length_mm + body_mass_g,     data = data, ks = min_rows(10, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.06006006\nBest kernel: optimal\nBest k: 10\n\n\nThis shows the type of response, an error rate estimate on the training set, kernel ( distance-weighting function used in the class vote), and choice of \\(k\\) (misleadingly listed as ‚ÄúBest k‚Äù). The kernel is actually another hyperparameter for this model, but parsnip selects a reasonable kernel by default. You can find the hyperparameters and their default values for a model on the model function‚Äôs help page. In this case, that‚Äôs ?nearest_neighbor.\nOnce you‚Äôve trained a model, you can use it with the predict function to make predictions. The predict function takes the fitted model and a data set with the same columns (excluding the response) as the training set as arguments. The function returns the predictions in a 1-column data frame.\nTo try out predict, let‚Äôs calculate the error on the training set manually. First, predict the class for each observation in the training set:\n\npreds = predict(fitted, peng)\n\npreds\n\n# A tibble: 333 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 Adelie     \n 2 Adelie     \n 3 Adelie     \n 4 Adelie     \n 5 Adelie     \n 6 Adelie     \n 7 Adelie     \n 8 Adelie     \n 9 Adelie     \n10 Adelie     \n# ‚Ñπ 323 more rows\n\n\nNotice that predict returns a data frame with a single column named .pred_class. The return value is always a data frame with a .pred_class column, regardless of the model used.\nNow compute the proportion of incorrect predictions:\n\nsum(peng$species != preds$.pred_class) / nrow(peng)\n\n[1] 0.04504505\n\n\nSince the model was trained on these penguins, this error rate likely underestimates the error rate for new penguins. Moreover, this is an overall error rate and doesn‚Äôt tell us much about error rates for the individual species.\nWe can get a slightly better idea of the model‚Äôs behavior by plotting the data again, marking the misclassified penguins:\n\nincorrect = peng$species != preds$.pred_class\n\nggplot(peng) + geom_point() +\n  aes(x = bill_length_mm, y = body_mass_g, shape = species,\n    color = incorrect) +\n  scale_color_manual(values = c(\"gray\", \"red\"))\n\n\n\n\n\n\n\n\nThe plot shows that most of the misclassified penguins are in regions where one group overlaps with another. We‚Äôll revisit this data set and model in subsequet examples, to learn about better evaluation methods and about strategies for improving the fit.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#selecting-a-model",
    "href": "chapters/01_supervised.html#selecting-a-model",
    "title": "1¬† Supervised Learning",
    "section": "1.3 Selecting a Model",
    "text": "1.3 Selecting a Model\n\n\n\nClassification boundaries for ten different models across three different data sets. Visualization by G. Varoquaux, A. M√ºller, & J. Grobler.\n\n\nThe No Free Lunch Theorem states that there is no single ‚Äúbest‚Äù supervised learning model. Different models make different assumptions, and will work better for data that satisfy those assumptions. Generally, you should select a model by comparing several different models and using whatever knowledge you have about the data.\n\n\n\n\n\n\nCaution\n\n\n\nThe wide variety of models people have invented can be distracting. For improving predictive performance, it‚Äôs almost always more effective to collect more data or to select or engineer better features than it is to switch models.\n\n\nIn spite of the No Free Lunch Theorem, some models are more general than others, and make good starting points. Here‚Äôs a list of some well-known classification models, where ‚ú® indicates a good starting point model:\n\n‚ú® Decision trees (classification tree guide, random forest guide)\n\nPros: easy to interpret, can handle missing data, selects features automatically\nCons: prone to overfitting, sensitive to class imbalance, training is computationally expensive\nTakeaway: random forests and boosted trees, which address overfitting at the cost of interpretability, perform well on a wide variety of problems\n\n‚ú® Generalized linear models (GLM) (logistic regression guide)\n\nPros: easy to interpret, training is computationally cheap, well-understood statistical properties, robust on small data sets\nCons: moderate assumptions (linearity)\nTakeaway: GLMs have a long history but remain effective, especially for smaller data sets\n\n‚ú® \\(k\\)-nearest neighbors (kNN)\n\nPros: conceptually simple\nCons: prediction is computationally expensive, poor performance at high number of features\nTakeaway: appropriate for small to medium data sets\n\nNaive Bayes\n\nPros: conceptually simple, scales well\nCons: strong assumptions (independence), only for classification\nTakeaway: useful for very large data sets\n\nSupport vector machines (SVM)\n\nPros: okay with high number of features\nCons: hard to interpret, conceptually complex, binary by default\nTakeaway: SVMs have become less popular than tree models in recent years, but can still be effective for many problems\n\nDiscriminant analysis (DA)\n\nPros: well-understood statistical properties\nCons: strong assumptions (normality), only for classification\nTakeaway: there are better models\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor another perspective on selecting a model, see the scikit-learn algorithm cheat sheet.\nAlthough scikit-learn is a Python package, its user guide is also an excellent, concise reference for machine learning concepts (without any code).\n\n\nFor most models, there‚Äôs a tradeoff between simplicity (in the sense of fewer parameters) and flexibility.\nSimple models make strong assumptions about the data, so there are fewer parameters to estimate. If the assumptions are satisfied, simple models perform well even when the number of observations is low or the observations are relatively noisy. However, if the assumptions aren‚Äôt satisfied, simple models tend to underfit, meaning they fail to learn the signal in the training data. Error in an underfit model is mostly bias caused by the assumptions.\n\n\n\n\n\n\nImportant\n\n\n\nBias is a specific statistical term which doesn‚Äôt carry the same connotation of unfairness as the common English meaning. Instead, it means that on average, a model‚Äôs predicted values differ from the actual values.\nWhether or not statistical bias makes a model unfair depends on the context in which the model will be used. For contexts where it doesn‚Äôt, there are often good reasons to accept some bias.\n\n\nFlexible models make only weak assumptions about the data. If the number of observations is high and the observations are not too noisy, flexible models typically perform well. However, if the number of observations is low or the observations are very noisy, flexible models tend to overfit, learning the noise in the training data instead of the signal. As a result, the model will not generalize well to new observations. Error in an overfit model is mostly due to variance, meaning the model is too sensitive to small changes in the training set.\nSimple versus flexible is spectrum, and models usually provide hyperparameters to control how simple or flexible the model is. One example is \\(k\\) in \\(k\\)-nearest neighbors. For \\(k\\) close to the number of observations, the model is simple and biased in favor of the majority class. For \\(k\\) close to 1, the model is flexible: it uses only a few observations in the training set to make each prediction, and thus is susceptible to noisy or anomalous observations.\nThe tradeoff between simple and flexible models is more commonly known as the bias-variance tradeoff. You can learn more about the bias-variance tradeoff from this interactive demo.\n\n1.3.1 Example: Bias-Variance Tradeoff\nSimulations are a great way to answer questions about or demonstrate properties of machine learning models. Let‚Äôs use a simulation to demonstrate the bias-variance tradeoff.\nFirst, let‚Äôs generate some data along this curve:\n\\[\ny = x^3 - 10 e^{-0.5 (x - 3)^2} + 1\n\\]\nAny non-polynomial curve will do, but we‚Äôll use this one because it plots nicely. To generate points for \\(x\\) in \\([-1, 1]\\) run:\n\nn = 80\nx = runif(n, -1, 1)\ny = x^3 - 10 * exp(-0.5 * (x - 3)^2) + 1\n\nNow let‚Äôs make several copies of these points, with random noise added to the \\(y\\) coordinates, to simulate collecting multiple samples with measurement error. To simulate 20 samples:\n\nn_samples = 20\n\nx = rep(x, n_samples)\n\nnoise = rnorm(n * n_samples, 0, 0.05)\ny = rep(y, n_samples) + noise\n\n# Store the points in a data frame with sample ID.\nsample_id = rep(seq(n_samples), each = n)\ndata = data.frame(x = x, y = y, sample_id = sample_id)\n\nBefore we fit any models, let‚Äôs plot one of the samples to see what it looks like:\n\nsample01 = subset(data, sample_id == 1)\n\nggplot(sample01) + aes(x = x, y = y) + geom_point()\n\n\n\n\n\n\n\n\nNow let‚Äôs fit two models to each sample:\n\nSince the data look somewhat quadratic, we‚Äôll fit a linear model with \\(x\\) and \\(x^2\\) as features. This will be our simple, high-bias model.\nWe‚Äôll also fit a model that‚Äôs just the mean of every 3 points. Although this model is conceptually simple, it will be our flexible, high-variance model.\n\nWe‚Äôll use the following function to fit the models. Note that it uses the zoo package‚Äôs rollmean function to fit the 3-point mean model, where ‚Äúfit‚Äù really just means computing the means. For the linear model, the function fits the model with lm and then computes points on the regression line for a grid of \\(x\\) coordinates.\n\n# install.packages(\"zoo\")\nlibrary(\"zoo\")\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nfit_models = function(samp) {\n  # Sort the points by x coordinate.\n  samp = samp[order(samp$x), ]\n\n  # Fit the linear model.\n  fitted_lm = lm(y ~ 1 + x + I(x^2), samp)\n\n  # Evaluate the linear model on a grid.\n  grid = seq(min(samp$x), max(samp$x), length.out = 1000)\n  lm_pts = data.frame(x = grid)\n  lm_pts$yhat = predict(fitted_lm, lm_pts)\n  lm_pts$model = \"linear\"\n\n  # Fit the 3-point mean model.\n  mean3_pts = data.frame(\n    # All x except first and last.\n    x = samp$x[-c(1, nrow(samp))],\n    yhat = rollmean(samp$y, 3),\n    model = \"3-point mean\"\n  )\n\n  # Combine into a single data frame.\n  fitted = rbind(lm_pts, mean3_pts)\n  fitted$sample_id = samp$sample_id[[1]]\n  fitted\n}\n\nWe can us the fit_models with split and lapply to fit the models to each sample, and then combine all of the predictions into a single data frame:\n\nby_sample = split(data, data$sample_id)\n\nfitted = lapply(by_sample, fit_models)\nfitted = do.call(rbind, fitted)\n\nFinally, let‚Äôs plot the fitted models:\n\nggplot(fitted) +\n  # Plot the points from one sample as an example.\n  geom_point(aes(x = x, y = y), sample01, color = \"gray\") +\n  # Plot the fitted models.\n  geom_line(aes(x = x, y = yhat, group = sample_id, color = factor(sample_id)),\n    alpha = 0.5) +\n  facet_wrap(vars(model)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nThe plot shows that the 3-sample mean model varies far more across samples than the linear model. In other words, the 3-sample mean model has high variance. The linear model is more stable, but doesn‚Äôt follow the training data as well‚Äîit has high bias.\nSimulations like this one are an important tool in every machine learning practicioner‚Äôs toolkit. You can use them to get a rough idea of how a model or estimator will behave under specific conditions. This example is based on one from Feature Engineering and Selection: A Practical Approach for Predictive Models by M. Kuhn & K. Johnson.\n\n\n1.3.2 Example: Four Penguin Models\nLet‚Äôs revisit the Palmer penguins data from Section 1.2.1. This time, we‚Äôll fit several different models, in order to examine how much accuracy differs between them.\nIn order to get reliable accuracy estimates, let‚Äôs randomly split the data into two parts: a training set and a test set. The training set will contain 75% of the observations and the test set will contain the remaining 25%. We‚Äôll train the models on the training set, and estimate accuracy on the test set. This ensures that we won‚Äôt overestimate the accuracy.\nSplitting data into a training set and test set is a common practice. The rsample package, which is included in tidymodels, provides an initial_split function to split a data set. The default split is 75-25. You can use the accompanying training and testing functions to get the training set and test set from the split object:\n\nlibrary(\"rsample\")\n\n# Set a seed to make the split reproducible.\nset.seed(10332)\nsplitted = initial_split(peng)\npeng_train = training(splitted)\npeng_test = testing(splitted)\n\nNow let‚Äôs fit the models and compute error estimates. This time, we‚Äôll use all four numeric features in the data set: bill length, bill depth, flipper length, and body mass. We‚Äôll use the default hyperparameters for each model except \\(k\\)-nearest neighbors, where there is no default for \\(k\\). We‚Äôll use \\(k = 15\\), since a rule of thumb for choosing \\(k\\) is to use \\(\\sqrt{n}\\), where \\(n\\) is the number of observations in the training set. For the penguins data:\n\nsqrt(nrow(peng_train))\n\n[1] 15.77973\n\n\nTo fit the models, run:\n\nmodels = list(\n  # k-nearest neighbors\n  knn = nearest_neighbor(\"classification\", neighbors = 15),\n  # Classification tree\n  tree = decision_tree(\"classification\"),\n  # Multinomial regression (a linear model)\n  multinomial = multinom_reg(\"classification\"),\n  # Support vector machine\n  svm = svm_rbf(\"classification\")\n)\n\nform =\n  species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g\n\nsapply(models, function(model) {\n  fitted = fit(model, form, peng_train)\n\n  predicted = predict(fitted, peng_test)$.pred_class\n  sum(peng_test$species == predicted) / nrow(peng_test)\n})\n\n        knn        tree multinomial         svm \n  0.9880952   0.9285714   0.9761905   0.9761905 \n\n\nAll of the models predict with accuracy above 92%, and three are within 1 of 97%. This example shows that for this data set, the choice of model has a relatively minor effect on accuracy. That‚Äôs the case for many data sets. Having enough observations and selecting or engineering relevant features is far more important.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#selecting-features",
    "href": "chapters/01_supervised.html#selecting-features",
    "title": "1¬† Supervised Learning",
    "section": "1.4 Selecting Features",
    "text": "1.4 Selecting Features\n\n\n\nIn a typical data science workflow, modeling is just one step among many and depends on the other steps. As with writing essays, modeling is usually a process of iterative refinement.\n\n\nIn practice, the choice of method has less impact on predictive performance than the features and number of observations in the training data. Different methods do make different assumptions, and it‚Äôs important to think through whether the assumptions are satisfied, but no method is a substitute for data.\nSome models, such as random forest models, can automatically select the most relevant features for prediction in a data set. With generalized linear models, you can use regularization methods, such as LASSO or Elastic Net, which add a penalty to the model‚Äôs loss function to encourage zeroing coefficients for unimportant features. These techniques can be helpful for screening features even if you ultimately plan to use a different model. You can also use exploratory analysis to assess which features have predictive power, and should use any expert knowledge you have about the data or population from which it was sampled.\nTransforming features to create new features, a process known as feature engineering, can also improve model performance. Unfortunately, it can be difficult to determine which features to engineer unless you have deep knowledge about the data and problem you‚Äôre modeling.\n\n\n\n\n\n\nTip\n\n\n\nFeature Engineering and Selection: A Practical Approach for Predictive Models by M. Kuhn & K. Johnson is a clear and thorough reference.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#sec-evaluating-models",
    "href": "chapters/01_supervised.html#sec-evaluating-models",
    "title": "1¬† Supervised Learning",
    "section": "1.5 Evaluating Models",
    "text": "1.5 Evaluating Models\nPerhaps the most informative classification model performance statistic is a confusion matrix, a cross-tabulation of actual and predicted classes. Unlike an accuracy estimate, a confusion matrix shows how well the model performs for each class. You can also use the confusion matrix to compute a variety of other statistics:\n\n\n\nStatistics you can use to evaluate classifier performance when there are two classes. Most of these generalize to more classes. Visualization inspired by Wikipedia‚Äôs diagnostic testing diagram.\n\n\nFour widely-used statistics are:\n\nPrecision, which is the proportion of predictions for a class that are correct. High precision for a given class indicates that a model is generally correct when it predicts the class, but might fail to recognize some members of the class.\nRecall or sensitivity, which is the proportion of a class‚Äô members that are correctly predicted. High recall for a given class indicates the model correctly recognizes members of the class, but might also predict members of other classes are in the class.\nSpecificity, which is recall for the second class in a two-class problem.\nThe F1 score, which is the harmonic mean of precision and recall. As a harmonic mean, the F1 score is close to 0 when precision or recall are and close to 1 when both precision and recall are.\n\nYou learn more about precision, recall, and the F1 score from this interactive guide.\nThe figure shows the situation for a two-class problem. You can also compute all of these statistics on a per-class basis when there are more than two classes (treat all other classes as the negative class). Moreover, you can compute overall precision, recall, and F1 score by averaging the class statistics. This is known as macro-averaging.\nAnother widely-used measure of performance for classification models is the receiver operating characteristic (ROC) curve and accompanying area under the curve (AUC). These measures are natural for classifiers that predict the probabilty of each class and use the predicted probabilities with a threshold decision rule to select the predicted class, because they show the behavior of the model for different choices of threshold. For models that do not predict class probabilities, the ROC curve does not exist, although researchers have devised ways to generalize the ROC curve to some of these models. You can learn more about ROC curves and AUC from this interactive guide.\n\n1.5.1 Partitioning Data\nIn Section 1.3.2, we split the Palmer penguins data set into two parts: a training set to use for training models and a testing set to use for testing models. Splitting data sets this way ensures that performance statistics computed on the testing set are accurate. If you compute performance statistics for a model with the same data set you used for training, the statisitics will have an optimistic bias. In other words, accuracy, precision, recall and other positive statistics will be overestimated, while error rate and other negative statistics will be underestimated.\nIt‚Äôs important that the testing set is not used to train models at any point in the modeling process. Even using the testing set to select models or hyperparameters can bias the testing set estimates.\nSince performance statistics are also helpful for selecting models and hyperparameters, and the training set will produce biased estimates, it‚Äôs often useful to split off a small part of the training set as a validation set. The validation set should only be used for selecting models and hyperparameters, not for training or final performance statistics. You can learn more about training, validation, and testing sets from this interactive guide.\nYou might wonder whether it‚Äôs inefficient to split the data set into so many subsets, since models generally perform better when they are trained on larger data sets. You‚Äôd be right to wonder‚Äîusing a fixed training set and validation set can create a pessimistic bias in the performance statistics computed on the validation set. It also makes the statistics sensitive to any outliers or anomalous observations that end up in the validation set.\n\\(k\\)-fold cross validition is one way to address the problems with using a fixed training set and validation set. In a \\(k\\)-fold cross-validation scheme, the training set is split into \\(k\\) random, equal-size subsets called folds. Then the model is fitted \\(k\\) times, giving each fold a chance to be the validation set (the remaining folds are used as a training set). This produces \\(k\\) estimates of any performance statistics of interest, which can be averaged to get an overall estimate. Cross-validation uses more of the data to train the model, which makes the performance statistic estimates more accurate. By averaging the estimates over several folds, it also ensures that the overall estimate is not too sensitive to unusual observations in a single fold. You can learn more about cross-validation from this interactive guide.\nThe main drawback of cross-validition is that the model must be trained more than once. For some models, training is computationally intensive, and training multiple times can be prohibitively so. For this reason, it‚Äôs common to set \\(k = 5\\) or \\(k = 10\\). Smaller values of \\(k\\) produce noisier estimates. An extreme case, known as leave-one-out cross-validation (LOOCV), sets \\(k\\) to the size of the training set, so that each observation is a fold. LOOCV is completely deterministic, whereas for any other \\(k\\) the cross-validation estimates vary depending on which observations end up in each fold.\n\n\n1.5.2 Example: Cross-Validated kNN\nLet‚Äôs revisit the Palmer penguins example that begin in Section 1.2.1 one more time. In the original example, we trained a \\(k\\)-nearest neighbors model and arbitrarily selected \\(k = 10\\). In Section 1.3.2, we selected \\(k = 15\\) instead based on a rule of thumb. In practice, the best way to select \\(k\\) is by estimating performance for several different values of \\(k\\). We can use cross validation to compute the estimates.\nThe rsample package provides a function vfold_cv to compute cross-validation folds. The function returns a data frame with the folds stored in the splits column. For each fold, you can use the training function to get the training set and the testing function to get the validation set.\nFor this example, let‚Äôs use 5-fold cross-validation to keep the computation time reasonable. Using more folds would give us more accurate estimates. We‚Äôll try all odd values of \\(k\\) from 1 to approximately the minimum number of observations in a training set:\n\n# Set a seed to make this reproducible.\nset.seed(1010)\n# 5-fold cross-validation\nfolds = vfold_cv(peng_train, 5)\n\n# Get a rough estimate of the minimum training set size.\nn_train = floor((4 / 5) * 0.75 * nrow(peng_train))\n# Try k = 1, 3, 5, ..., n_train.\nks = seq(1, n_train, 2)\n\nform =\n  species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing \\(k\\)-fold cross-validation this way can produce inaccurate estimates when there are more observations for some classes than others (as is the case here).\nIt would be better to use stratified \\(k\\)-fold cross-validation, which attempts to preserve the overall class proportions within each fold. You can do this with the vfold_cv function by setting the strata parameter to the feature you want to use for stratification (in this case, we would use species).\n\n\nNow we can use nested apply functions or loops to iterate over the \\(k\\) and folds. For each fold, we‚Äôll get the training set and train the model with the current \\(k\\), then predict on the test set. We can use the predictions to compute performance statistics. To keep the example simple, let‚Äôs compute overall accuracy, although in practice it‚Äôs important to look at other statistics, including per-class statistics. The code to carry out these steps is:\n\n# Loop over k values.\nerror_rates = sapply(ks, function(k) {\n  # Loop over folds.\n  sapply(folds$splits, function(fold) {\n    # Fit the model on the training set.\n    train = training(fold)\n    model = nearest_neighbor(\"classification\", neighbors = k,\n      weight_func = \"rectangular\")\n    fitted = fit(model, form, train)\n\n    # Predict on the test set.\n    test = testing(fold)\n    preds = predict(fitted, test)\n    sum(test$species != preds$.pred_class) / nrow(test)\n  })\n})\n\nThe result from the code is a matrix of estimates. Each row corresponds to one value of \\(k\\), while each column corresponds to one fold. Let‚Äôs compute an overall estimate for each \\(k\\) with the colMeans function, and then plot the estimates against \\(k\\).\n\nestimates = data.frame(\n  k = ks,\n  error = colMeans(error_rates)\n)\n\nggplot(estimates) + aes(x = k, y = error) + geom_line()\n\n\n\n\n\n\n\n\nFor this particular data set, it looks like \\(k = 1\\) provides the best accuracy. This may seem somewhat surprising because of the bias-variance tradeoff, and it is a good idea to double-check your code in situations like this. However, for this data set, each penguin species is so well-separated by the four features that \\(k = 1\\) really does work well. If we wanted the use the model for new samples of penguins, it might still be a good idea to choose a slightly higher \\(k\\) value just in case this data set happens to have less variation than the population.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html",
    "href": "chapters/02_unsupervised.html",
    "title": "2¬† Unsupervised Learning",
    "section": "",
    "text": "2.1 Packages for Unsupervised Learning",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html#packages-for-unsupervised-learning",
    "href": "chapters/02_unsupervised.html#packages-for-unsupervised-learning",
    "title": "2¬† Unsupervised Learning",
    "section": "",
    "text": "CRAN Task View: Cluster Analysis\ntidyclust",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  }
]