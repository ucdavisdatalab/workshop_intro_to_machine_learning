[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "Overview\nThis collection of workshops provides an introduction to machine learning. The collection has two standalone parts:\n\nOverview of Machine Learning (one 2-hour session): This is a non-technical workshop that emphasizes building vocabulary and gaining an intuitive understanding of machine learning concepts and methods. Start here if you‚Äôre new to machine learning and want to get a sense of what it‚Äôs about and whether it‚Äôs relevant to you. There‚Äôs no code in this workshop and only a little (high-school level) math. This workshop is also good preparation for the Machine Learning in R series.\n\n\n\n\n\n\nLearning Goals\n\n\n\n\n\nAfter completing this workshop, learners should be able to:\n\nDefine the following terms: observation, feature, machine learning, supervised learning, unsupervised learning, regression, classification, clustering, training set, validation set, test set, cross-validation, overfitting, underfitting, model bias, model variance, bias-variance tradeoff, ensemble model.\nExplain the difference between supervised and unsupervised learning.\nExplain the difference between regression and classification.\nList and briefly describe popular machine learning methods.\nGive an example of an ensemble model.\nExplain what cross-validation is used for and give an overview of the procedure.\nAssess whether and which machine learning methods might be helpful for a given research problem.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis slide deck is the only material for this workshop.\n\n\nMachine Learning in R (two 2-hour sessions): This is a hands-on, technical introduction to using machine learning methods in R. The two sessions cover and include examples of supervised learning (emphasis on classification), model evaluation, unsupervised learning (emphasis on clustering), and dimension reduction. The sessions also provide advice for navigating R‚Äôs fractured machine learning package landscape. Intermediate familiarity with R programming (equivalent to completing DataLab‚Äôs R Basics workshop series) is required.\n\n\n\n\n\n\nLearning Goals\n\n\n\n\n\nAfter completing this series, learners should be able to:\n\nBuild and train a classification model on their data.\nUse cross-validation to estimate accuracy and tune hyperparameters for classification models.\nIdentify strategies to improve results from classification models.\nExplain the tradeoffs between popular clustering algorithms.\nRun a clustering algorithm on their data.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html",
    "href": "chapters/01_supervised.html",
    "title": "1¬† Supervised Learning",
    "section": "",
    "text": "1.1 Introduction\nSupervised learning methods learn from training data in order to make predictions about new data.\nBefore going further, let‚Äôs review some necessary vocabulary:\nFor example, suppose you‚Äôve collected a data set with the age, height, and species of every tree in your neighborhood. Each tree is a unit. Each (age, height, species) measurement for a tree is an observation. The features are age, height, and species. If you want to use supervised learning to predict height based on age and species, then height is a response. üå≤üå≥üå¥\nProblems and methods are conventionally described in terms of the type of response:\nContinuing the trees example, if you want to predict height in meters, you have a regression problem. If you want to predict species, you have a classification problem.\nThis chapter focuses on classification, but many of the concepts covered here are equally relevant to regression. Moreover, there are regression variants of many classification methods. In fact, many classifiers are just a combination of a regression method that predicts class probabilities and a decision rule that selects a class based on the prediction.\nA supervised learning method consists of:\nMethods ‚Äúlearn‚Äù by selecting parameter values that minimize loss for predictions about training data, in the hope that these values will generalize well to new data.\nMost models also have hyperparameters (or tuning parameters), parameters that are not learned and must instead be specified before training. Usually, these control how much the model adapts to the training data. To select hyperparameter values, use your own knowledge about the data as well as model evaluation techniques (see Section 1.5).\nThe next section presents an overview of packages for supervised learning in R, some of which are demonstrated in later examples. Subsequent sections explain how to select features, how to select a model, and common model evaluation strategies, including data partitioning and cross-validation. The chapter ends with a short discussion of how to improve model performance and where to go to learn more.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#sec-supervised-learning",
    "href": "chapters/01_supervised.html#sec-supervised-learning",
    "title": "1¬† Supervised Learning",
    "section": "",
    "text": "A unit is an entity of interest in a data set. Units are sometimes also called subjects.\nAn observation is all measurements for a single unit. Observations are sometimes also called cases.\nA feature is a single measurement across all units. Features are sometimes also called variables, covariates, or predictors.\nA response is a feature that you want to predict.\nThe dimensionality of a data set is the number of features.\n\n\n\n\nRegression refers to a numerical response.\nClassification refers to a categorical response. We call the categories classes.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to learn more about regression, see DataLab‚Äôs Introduction to Regression Modeling in R workshop series.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nClassification methods are reductive: they assume that classes are mutually exclusive and only predict one class for each data point.\nIn contexts where predictions are meant to inform people making decisions, regression methods are often more appropriate and yield more informative predictions (for example, class probabilities). Frank Harrell (Professor of Biostatistics, Vanderbilt University) discusses this in his post Classification vs.¬†Prediction.\nOn the other hand, classification methods often work well for automating decisions or tasks that are straightforward but tedious, especially if the stakes are low.\n\n\n\n\nA model for making predictions, with assumptions and adjustable parameters that govern what predictions are made.\nA loss function for measuring the real or abstract cost of incorrect predictions.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#packages",
    "href": "chapters/01_supervised.html#packages",
    "title": "1¬† Supervised Learning",
    "section": "1.2 Packages",
    "text": "1.2 Packages\nThe R community has developed dozens of machine learning packages to supplement R‚Äôs small selection of built-in functions. A majority of these provide a single method or family of methods (such as decision trees).\nThe CRAN Task View: Machine Learning page is a good starting point for finding packages related to supervised learning. The task view is regularly updated by the CRAN administrators and only lists popular, actively-maintained packages.\n\n\n\n\n\n\n\nTip\n\n\n\nIf the task view doesn‚Äôt list a package for a method you want to use, try searching the web. Many packages hosted on CRAN or GitHub are not listed in the task view because they‚Äôre obscure, unmaintained, or in early development. None of these things mean a package won‚Äôt work, although you might want to check the package code carefully to make sure it does what you expect.\n\n\nDifferent packages generally provide different programming interfaces. This makes studying machine learning in R difficult: in addition to learning each method, you must also learn the idiosyncrasies of each package. This also makes it difficult to compare methods by swapping them in and out of a data processing pipeline. To address this, some community members have developed packages that provide a common programming interface on top of existing, independent methods packages. Others have developed omnibus packages that implement a wide variety of methods. These packages are listed below from most to least CRAN downloads in March 2024.\n\ncaret is a common interface for training models with existing packages, as well as a collection of functions for related tasks (such as splitting data).\ntidymodels is a metapackage (a collection of packages) for supervised learning designed to work well with tidyverse packages. In some respects, this is the successor to caret. Especially important are the packages:\n\nparsnip provides a common interface for existing methods packages, or ‚Äúengines‚Äù in the terminology of parsnip. For some methods, multiple engines are available, while for others, there is only one.\nrsample provides functions to partition and sample data sets.\nyardstick provides functions to compute a variety of performance statistics.\nbroom provides functions to summarize model results in data frames.\n\nh2o is an omnibus machine learning package, with implementations of many popular supervised and unsupervised learning methods. h2o is available for many different programming languages and places emphasis on computational efficiency.\nmlr3 and mlr3learners provide a common, object-oriented interface for existing methods packages.\nSuperLearner is an omnibus machine learning package designed to evaluate multiple methods and select the best combination of them.\nqeML is a relatively new package that provides a common interface for existing methods, intended to be easier to use than tidymodels and mlr3. qeML is developed by UC Davis Professor Emeritus Norm Matloff.\n\nThe caret, tidymodels, and h2o packages are all relatively complete solutions with detailed documentation, many users, and many contributors.\n\n1.2.1 Example: Classifying Penguins\n\n\n\nChinstrap, Gentoo, and Ad√©lie penguins. Artwork by Allison Horst.\n\n\nIn this example, we‚Äôll fit a \\(k\\)-nearest neighbors (kNN) classifier with the tidymodels package. In \\(k\\)-nearest neighbors, new observations are classified by taking a majority vote of the classes of the \\(k\\) nearest training set observations. Generally, we choose odd \\(k\\) to prevent ties, although there are schemes for breaking ties if \\(k\\) is even.\nAs an example data set, we‚Äôll use the Palmer Penguins data set, which was collected by Dr.¬†Kristen Gorman at Palmer Station, Antarctica. The data set records physical characteristics for hundreds of individual penguins from three different species: Ad√©lie, Chinstrap, and Gentoo.\nThe data set is available for R in the palmerpenguins package. We‚Äôll also use several other packages in this example. To install them all:\n\ninstall.packages(\"palmerpenguins\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"tidymodels\")\n\nLoad the palmerpenguins package, which will automatically create a penguins variable, and take a look at the first few observations:\n\nlibrary(\"palmerpenguins\")\n\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ‚Ñπ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nIt‚Äôs a good idea to do some exploratory analysis before fitting a model. We can use ggplot2 to plot bill length, body mass, and species for each penguin:\n\nlibrary(\"ggplot2\")\n\nplt = ggplot(penguins) + geom_point() + aes(shape = species, color = species)\n\nplt %+% aes(x = bill_length_mm, y = body_mass_g)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Other plots that might be of interest:\n# plt %+% aes(x = bill_length_mm, y = bill_depth_mm)\n# plt %+% aes(x = bill_length_mm, y = flipper_length_mm)\n# plt %+% aes(x = bill_depth_mm, y = body_mass_g)\n# plt %+% aes(x = bill_depth_mm, y = flipper_length_mm)\n# plt %+% aes(x = flipper_length_mm, y = body_mass_g)\n\nThe plot shows that the three species of penguins are relatively well-separated by bill length and body mass. Ad√©lie penguins have relatively short bills and light bodies, Chinstrap penguins have relatively long bills and light bodies, and Gentoo penguins have relatively long bills and heavy bodies. The species only overlap for a few penguins. This separation is a sign that a classifier is likely to work well.\n\n\n\n\n\n\nTip\n\n\n\nClassification is more difficult when the classes are not well-separated by the features, but not necessarily intractible. Sometimes it‚Äôs possible to transform features to provide better separation. Some models can also take advantage of interactions between features that are not readily apparent in scatter plots.\n\n\nInitial exploratory analysis is also when you should check data for outliers and missing values. The previous plot didn‚Äôt show any signs of extreme outliers. Now let‚Äôs check for missing values:\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nkNN classifiers can‚Äôt handle missing values, and only a few observation have them, so let‚Äôs remove these observations from the data set:\n\npeng = na.omit(penguins)\n\nNow let‚Äôs train a kNN classifier with tidymodels. Specifically, we‚Äôll use the parsnip package, which is included with tidymodels and provides a common interface for a wide variety of supervised learning models.\nTraining with parsnip follows the same two steps for every model:\n\nlibrary(\"parsnip\")\n\n1knn = nearest_neighbor(\"classification\", neighbors = 10)\n2fitted = fit(knn, species ~ bill_length_mm + body_mass_g, peng)\n\n\n1\n\nInitialize a model and set hyperparameters by calling a model function. In this case, we use nearest_neighbor since we want a kNN model, and set the number of neighbors to 10.\n\n2\n\nTrain the model by calling fit with the model, a formula ~ that specifies the response on the left-hand side and the predictors on the right-hand side, and the data.\n\n\n\n\nYou can print the fitted model to get more information:\n\nfitted\n\nparsnip model object\n\n\nCall:\nkknn::train.kknn(formula = species ~ bill_length_mm + body_mass_g,     data = data, ks = min_rows(10, data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.06006006\nBest kernel: optimal\nBest k: 10\n\n\nThis shows the type of response, an error rate estimate on the training set, kernel ( distance-weighting function used in the class vote), and choice of \\(k\\) (misleadingly listed as ‚ÄúBest k‚Äù). The kernel is actually another hyperparameter for this model, but parsnip selects a reasonable kernel by default. You can find the hyperparameters and their default values for a model on the model function‚Äôs help page. In this case, that‚Äôs ?nearest_neighbor.\nOnce you‚Äôve trained a model, you can use it with the predict function to make predictions. The predict function takes the fitted model and a data set with the same columns (excluding the response) as the training set as arguments. The function returns the predictions in a 1-column data frame.\nTo try out predict, let‚Äôs calculate the error on the training set manually. First, predict the class for each observation in the training set:\n\npreds = predict(fitted, peng)\n\npreds\n\n# A tibble: 333 √ó 1\n   .pred_class\n   &lt;fct&gt;      \n 1 Adelie     \n 2 Adelie     \n 3 Adelie     \n 4 Adelie     \n 5 Adelie     \n 6 Adelie     \n 7 Adelie     \n 8 Adelie     \n 9 Adelie     \n10 Adelie     \n# ‚Ñπ 323 more rows\n\n\nNotice that predict returns a data frame with a single column named .pred_class. The return value is always a data frame with a .pred_class column, regardless of the model used.\nNow compute the proportion of incorrect predictions:\n\nsum(peng$species != preds$.pred_class) / nrow(peng)\n\n[1] 0.04504505\n\n\nSince the model was trained on these penguins, this error rate likely underestimates the error rate for new penguins. Moreover, this is an overall error rate and doesn‚Äôt tell us much about error rates for the individual species.\nWe can get a slightly better idea of the model‚Äôs behavior by plotting the data again, marking the misclassified penguins:\n\nincorrect = peng$species != preds$.pred_class\n\nggplot(peng) + geom_point() +\n  aes(x = bill_length_mm, y = body_mass_g, shape = species,\n    color = incorrect) +\n  scale_color_manual(values = c(\"gray\", \"red\"))\n\n\n\n\n\n\n\n\nThe plot shows that most of the misclassified penguins are in regions where one group overlaps with another. We‚Äôll revisit this data set and model in subsequet examples, to learn about better evaluation methods and about strategies for improving the fit.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#selecting-a-model",
    "href": "chapters/01_supervised.html#selecting-a-model",
    "title": "1¬† Supervised Learning",
    "section": "1.3 Selecting a Model",
    "text": "1.3 Selecting a Model\n\n\n\nClassification boundaries for ten different models across three different data sets. Visualization by G. Varoquaux, A. M√ºller, & J. Grobler.\n\n\nThe No Free Lunch Theorem states that there is no single ‚Äúbest‚Äù supervised learning model. Different models make different assumptions, and will work better for data that satisfy those assumptions. Generally, you should select a model by comparing several different models and using whatever knowledge you have about the data.\n\n\n\n\n\n\nCaution\n\n\n\nThe wide variety of models people have invented can be distracting. For improving predictive performance, it‚Äôs almost always more effective to collect more data or to select or engineer better features than it is to switch models.\n\n\nIn spite of the No Free Lunch Theorem, some models are more general than others, and make good starting points. Here‚Äôs a list of some well-known classification models, where ‚ú® indicates a good starting point model:\n\n‚ú® Decision trees (classification tree guide, random forest guide)\n\nPros: easy to interpret, can handle missing data, selects features automatically\nCons: prone to overfitting, sensitive to class imbalance, training is computationally expensive\nTakeaway: random forests and boosted trees, which address overfitting at the cost of interpretability, perform well on a wide variety of problems\n\n‚ú® Generalized linear models (GLM) (logistic regression guide)\n\nPros: easy to interpret, training is computationally cheap, well-understood statistical properties, robust on small data sets\nCons: moderate assumptions (linearity)\nTakeaway: GLMs have a long history but remain effective, especially for smaller data sets\n\n‚ú® \\(k\\)-nearest neighbors (kNN)\n\nPros: conceptually simple\nCons: prediction is computationally expensive, poor performance at high number of features\nTakeaway: appropriate for small to medium data sets\n\nNaive Bayes\n\nPros: conceptually simple, scales well\nCons: strong assumptions (independence), only for classification\nTakeaway: useful for very large data sets\n\nSupport vector machines (SVM)\n\nPros: okay with high number of features\nCons: hard to interpret, conceptually complex, binary by default\nTakeaway: SVMs have become less popular than tree models in recent years, but can still be effective for many problems\n\nDiscriminant analysis (DA)\n\nPros: well-understood statistical properties\nCons: strong assumptions (normality), only for classification\nTakeaway: there are better models\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor another perspective on selecting a model, see the scikit-learn algorithm cheat sheet.\nAlthough scikit-learn is a Python package, its user guide is also an excellent, concise reference for machine learning concepts (without any code).\n\n\nFor most models, there‚Äôs a tradeoff between simplicity (in the sense of fewer parameters) and flexibility.\nSimple models make strong assumptions about the data, so there are fewer parameters to estimate. If the assumptions are satisfied, simple models perform well even when the number of observations is low or the observations are relatively noisy. However, if the assumptions aren‚Äôt satisfied, simple models tend to underfit, meaning they fail to learn the signal in the training data. Error in an underfit model is mostly bias caused by the assumptions.\n\n\n\n\n\n\nImportant\n\n\n\nBias is a specific statistical term which doesn‚Äôt carry the same connotation of unfairness as the common English meaning. Instead, it means that on average, a model‚Äôs predicted values differ from the actual values.\nWhether or not statistical bias makes a model unfair depends on the context in which the model will be used. For contexts where it doesn‚Äôt, there are often good reasons to accept some bias.\n\n\nFlexible models make only weak assumptions about the data. If the number of observations is high and the observations are not too noisy, flexible models typically perform well. However, if the number of observations is low or the observations are very noisy, flexible models tend to overfit, learning the noise in the training data instead of the signal. As a result, the model will not generalize well to new observations. Error in an overfit model is mostly due to variance, meaning the model is too sensitive to small changes in the training set.\nSimple versus flexible is spectrum, and models usually provide hyperparameters to control how simple or flexible the model is. One example is \\(k\\) in \\(k\\)-nearest neighbors. For \\(k\\) close to the number of observations, the model is simple and biased in favor of the majority class. For \\(k\\) close to 1, the model is flexible: it uses only a few observations in the training set to make each prediction, and thus is susceptible to noisy or anomalous observations.\nThe tradeoff between simple and flexible models is more commonly known as the bias-variance tradeoff. You can learn more about the bias-variance tradeoff from this interactive demo.\n\n1.3.1 Example: Bias-Variance Tradeoff\nSimulations are a great way to answer questions about or demonstrate properties of machine learning models. Let‚Äôs use a simulation to demonstrate the bias-variance tradeoff.\nFirst, let‚Äôs generate some data along this curve:\n\\[\ny = x^3 - 10 e^{-0.5 (x - 3)^2} + 1\n\\]\nAny non-polynomial curve will do, but we‚Äôll use this one because it plots nicely. To generate points for \\(x\\) in \\([-1, 1]\\) run:\n\nn = 80\nx = runif(n, -1, 1)\ny = x^3 - 10 * exp(-0.5 * (x - 3)^2) + 1\n\nNow let‚Äôs make several copies of these points, with random noise added to the \\(y\\) coordinates, to simulate collecting multiple samples with measurement error. To simulate 20 samples:\n\nn_samples = 20\n\nx = rep(x, n_samples)\n\nnoise = rnorm(n * n_samples, 0, 0.05)\ny = rep(y, n_samples) + noise\n\n# Store the points in a data frame with sample ID.\nsample_id = rep(seq(n_samples), each = n)\ndata = data.frame(x = x, y = y, sample_id = sample_id)\n\nBefore we fit any models, let‚Äôs plot one of the samples to see what it looks like:\n\nsample01 = subset(data, sample_id == 1)\n\nggplot(sample01) + aes(x = x, y = y) + geom_point()\n\n\n\n\n\n\n\n\nNow let‚Äôs fit two models to each sample:\n\nSince the data look somewhat quadratic, we‚Äôll fit a linear model with \\(x\\) and \\(x^2\\) as features. This will be our simple, high-bias model.\nWe‚Äôll also fit a model that‚Äôs just the mean of every 3 points. Although this model is conceptually simple, it will be our flexible, high-variance model.\n\nWe‚Äôll use the following function to fit the models. Note that it uses the zoo package‚Äôs rollmean function to fit the 3-point mean model, where ‚Äúfit‚Äù really just means computing the means. For the linear model, the function fits the model with lm and then computes points on the regression line for a grid of \\(x\\) coordinates.\n\n# install.packages(\"zoo\")\nlibrary(\"zoo\")\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nfit_models = function(samp) {\n  # Sort the points by x coordinate.\n  samp = samp[order(samp$x), ]\n\n  # Fit the linear model.\n  fitted_lm = lm(y ~ 1 + x + I(x^2), samp)\n\n  # Evaluate the linear model on a grid.\n  grid = seq(min(samp$x), max(samp$x), length.out = 1000)\n  lm_pts = data.frame(x = grid)\n  lm_pts$yhat = predict(fitted_lm, lm_pts)\n  lm_pts$model = \"linear\"\n\n  # Fit the 3-point mean model.\n  mean3_pts = data.frame(\n    # All x except first and last.\n    x = samp$x[-c(1, nrow(samp))],\n    yhat = rollmean(samp$y, 3),\n    model = \"3-point mean\"\n  )\n\n  # Combine into a single data frame.\n  fitted = rbind(lm_pts, mean3_pts)\n  fitted$sample_id = samp$sample_id[[1]]\n  fitted\n}\n\nWe can us the fit_models with split and lapply to fit the models to each sample, and then combine all of the predictions into a single data frame:\n\nby_sample = split(data, data$sample_id)\n\nfitted = lapply(by_sample, fit_models)\nfitted = do.call(rbind, fitted)\n\nFinally, let‚Äôs plot the fitted models:\n\nggplot(fitted) +\n  # Plot the points from one sample as an example.\n  geom_point(aes(x = x, y = y), sample01, color = \"gray\") +\n  # Plot the fitted models.\n  geom_line(aes(x = x, y = yhat, group = sample_id, color = factor(sample_id)),\n    alpha = 0.5) +\n  facet_wrap(vars(model)) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\nThe plot shows that the 3-sample mean model varies far more across samples than the linear model. In other words, the 3-sample mean model has high variance. The linear model is more stable, but doesn‚Äôt follow the training data as well‚Äîit has high bias.\nSimulations like this one are an important tool in every machine learning practicioner‚Äôs toolkit. You can use them to get a rough idea of how a model or estimator will behave under specific conditions. This example is based on one from Feature Engineering and Selection: A Practical Approach for Predictive Models by M. Kuhn & K. Johnson.\n\n\n1.3.2 Example: Four Penguin Models\nLet‚Äôs revisit the Palmer penguins data from Section 1.2.1. This time, we‚Äôll fit several different models, in order to examine how much accuracy differs between them.\nIn order to get reliable accuracy estimates, let‚Äôs randomly split the data into two parts: a training set and a test set. The training set will contain 75% of the observations and the test set will contain the remaining 25%. We‚Äôll train the models on the training set, and estimate accuracy on the test set. This ensures that we won‚Äôt overestimate the accuracy.\nSplitting data into a training set and test set is a common practice. The rsample package, which is included in tidymodels, provides an initial_split function to split a data set. The default split is 75-25. You can use the accompanying training and testing functions to get the training set and test set from the split object:\n\nlibrary(\"rsample\")\n\n# Set a seed to make the split reproducible.\nset.seed(10332)\nsplitted = initial_split(peng)\npeng_train = training(splitted)\npeng_test = testing(splitted)\n\nNow let‚Äôs fit the models and compute error estimates. This time, we‚Äôll use all four numeric features in the data set: bill length, bill depth, flipper length, and body mass. We‚Äôll use the default hyperparameters for each model except \\(k\\)-nearest neighbors, where there is no default for \\(k\\). We‚Äôll use \\(k = 15\\), since a rule of thumb for choosing \\(k\\) is to use \\(\\sqrt{n}\\), where \\(n\\) is the number of observations in the training set. For the penguins data:\n\nsqrt(nrow(peng_train))\n\n[1] 15.77973\n\n\nTo fit the models, run:\n\nmodels = list(\n  # k-nearest neighbors\n  knn = nearest_neighbor(\"classification\", neighbors = 15),\n  # Classification tree\n  tree = decision_tree(\"classification\"),\n  # Multinomial regression (a linear model)\n  multinomial = multinom_reg(\"classification\"),\n  # Support vector machine\n  svm = svm_rbf(\"classification\")\n)\n\nform =\n  species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g\n\nsapply(models, function(model) {\n  fitted = fit(model, form, peng_train)\n\n  predicted = predict(fitted, peng_test)$.pred_class\n  sum(peng_test$species == predicted) / nrow(peng_test)\n})\n\n        knn        tree multinomial         svm \n  0.9880952   0.9285714   0.9761905   0.9761905 \n\n\nAll of the models predict with accuracy above 92%, and three are within 1 of 97%. This example shows that for this data set, the choice of model has a relatively minor effect on accuracy. That‚Äôs the case for many data sets. Having enough observations and selecting or engineering relevant features is far more important.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#selecting-features",
    "href": "chapters/01_supervised.html#selecting-features",
    "title": "1¬† Supervised Learning",
    "section": "1.4 Selecting Features",
    "text": "1.4 Selecting Features\n\n\n\nIn a typical data science workflow, modeling is just one step among many and depends on the other steps. As with writing essays, modeling is usually a process of iterative refinement.\n\n\nIn practice, the choice of method has less impact on predictive performance than the features and number of observations in the training data. Different methods do make different assumptions, and it‚Äôs important to think through whether the assumptions are satisfied, but no method is a substitute for data.\nSome models, such as random forest models, can automatically select the most relevant features for prediction in a data set. With generalized linear models, you can use regularization methods, such as LASSO or Elastic Net, which add a penalty to the model‚Äôs loss function to encourage zeroing coefficients for unimportant features. These techniques can be helpful for screening features even if you ultimately plan to use a different model. You can also use exploratory analysis to assess which features have predictive power, and should use any expert knowledge you have about the data or population from which it was sampled.\nTransforming features to create new features, a process known as feature engineering, can also improve model performance. Unfortunately, it can be difficult to determine which features to engineer unless you have deep knowledge about the data and problem you‚Äôre modeling.\n\n\n\n\n\n\nTip\n\n\n\nFeature Engineering and Selection: A Practical Approach for Predictive Models by M. Kuhn & K. Johnson is a clear and thorough reference.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/01_supervised.html#sec-evaluating-models",
    "href": "chapters/01_supervised.html#sec-evaluating-models",
    "title": "1¬† Supervised Learning",
    "section": "1.5 Evaluating Models",
    "text": "1.5 Evaluating Models\n\n\n\n\n\n\nImportant\n\n\n\nThis section emphasizes ways to evaluate model performance, or how effectively a model makes predictions. Carefully thinking through the context, purpose, ethics, and fairness of a model is even more important, if less technical.\nA few useful references for thinking about model context and purpose are:\n\nUC Berkeley‚Äôs Human Contexts & Ethics Toolkit\nDataLab‚Äôs Responsible Data Science reader\nThe Turing Way‚Äôs Guide for Ethical Research\n\nResearchers have invented many different ways to quantify fairness. Arvind Narayanan (Professor of Computer Science, Princeton University) presented a lecture that introduces 21 of these and also co-authored a textbook about fairness in machine learning.\n\n\nPerhaps the most informative classification model performance statistic is a confusion matrix, a cross-tabulation of actual and predicted classes. Unlike an accuracy estimate, a confusion matrix shows how well the model performs for each class. You can also use the confusion matrix to compute a variety of other statistics:\n\n\n\nStatistics you can use to evaluate classifier performance when there are two classes. Most of these generalize to more classes. Visualization inspired by Wikipedia‚Äôs diagnostic testing diagram.\n\n\nFour widely-used statistics are:\n\nPrecision, which is the proportion of predictions for a class that are correct. High precision for a given class indicates that a model is generally correct when it predicts the class, but might fail to recognize some members of the class.\nRecall or sensitivity, which is the proportion of a class‚Äô members that are correctly predicted. High recall for a given class indicates the model correctly recognizes members of the class, but might also predict members of other classes are in the class.\nSpecificity, which is recall for the second class in a two-class problem.\nThe F1 score, which is the harmonic mean of precision and recall. As a harmonic mean, the F1 score is close to 0 when precision or recall are and close to 1 when both precision and recall are.\n\nYou learn more about precision, recall, and the F1 score from this interactive guide.\nThe figure shows the situation for a two-class problem. You can also compute all of these statistics on a per-class basis when there are more than two classes (treat all other classes as the negative class). Moreover, you can compute overall precision, recall, and F1 score by averaging the class statistics. This is known as macro-averaging.\nAnother widely-used measure of performance for classification models is the receiver operating characteristic (ROC) curve and accompanying area under the curve (AUC). These measures are natural for classifiers that predict the probabilty of each class and use the predicted probabilities with a threshold decision rule to select the predicted class, because they show the behavior of the model for different choices of threshold. For models that do not predict class probabilities, the ROC curve does not exist, although researchers have devised ways to generalize the ROC curve to some of these models. You can learn more about ROC curves and AUC from this interactive guide.\n\n1.5.1 Partitioning Data\nIn Section 1.3.2, we split the Palmer penguins data set into two parts: a training set to use for training models and a testing set to use for testing models. Splitting data sets this way ensures that performance statistics computed on the testing set are accurate. If you compute performance statistics for a model with the same data set you used for training, the statisitics will have an optimistic bias. In other words, accuracy, precision, recall and other positive statistics will be overestimated, while error rate and other negative statistics will be underestimated.\nIt‚Äôs important that the testing set is not used to train models at any point in the modeling process. Even using the testing set to select models or hyperparameters can bias the testing set estimates.\nSince performance statistics are also helpful for selecting models and hyperparameters, and the training set will produce biased estimates, it‚Äôs often useful to split off a small part of the training set as a validation set. The validation set should only be used for selecting models and hyperparameters, not for training or final performance statistics. You can learn more about training, validation, and testing sets from this interactive guide.\nYou might wonder whether it‚Äôs inefficient to split the data set into so many subsets, since models generally perform better when they are trained on larger data sets. You‚Äôd be right to wonder‚Äîusing a fixed training set and validation set can create a pessimistic bias in the performance statistics computed on the validation set. It also makes the statistics sensitive to any outliers or anomalous observations that end up in the validation set.\n\\(k\\)-fold cross validition is one way to address the problems with using a fixed training set and validation set. In a \\(k\\)-fold cross-validation scheme, the training set is split into \\(k\\) random, equal-size subsets called folds. Then the model is fitted \\(k\\) times, giving each fold a chance to be the validation set (the remaining folds are used as a training set). This produces \\(k\\) estimates of any performance statistics of interest, which can be averaged to get an overall estimate. Cross-validation uses more of the data to train the model, which makes the performance statistic estimates more accurate. By averaging the estimates over several folds, it also ensures that the overall estimate is not too sensitive to unusual observations in a single fold. You can learn more about cross-validation from this interactive guide.\nThe main drawback of cross-validition is that the model must be trained more than once. For some models, training is computationally intensive, and training multiple times can be prohibitively so. For this reason, it‚Äôs common to set \\(k = 5\\) or \\(k = 10\\). Smaller values of \\(k\\) produce noisier estimates. An extreme case, known as leave-one-out cross-validation (LOOCV), sets \\(k\\) to the size of the training set, so that each observation is a fold. LOOCV is completely deterministic, whereas for any other \\(k\\) the cross-validation estimates vary depending on which observations end up in each fold.\n\n\n1.5.2 Example: Cross-Validated kNN\nLet‚Äôs revisit the Palmer penguins example that begin in Section 1.2.1 one more time. In the original example, we trained a \\(k\\)-nearest neighbors model and arbitrarily selected \\(k = 10\\). In Section 1.3.2, we selected \\(k = 15\\) instead based on a rule of thumb. In practice, the best way to select \\(k\\) is by estimating performance for several different values of \\(k\\). We can use cross validation to compute the estimates.\nThe rsample package provides a function vfold_cv to compute cross-validation folds. The function returns a data frame with the folds stored in the splits column. For each fold, you can use the training function to get the training set and the testing function to get the validation set.\nFor this example, let‚Äôs use 5-fold cross-validation to keep the computation time reasonable. Using more folds would give us more accurate estimates. We‚Äôll try all odd values of \\(k\\) from 1 to approximately the minimum number of observations in a training set:\n\n# Set a seed to make this reproducible.\nset.seed(1010)\n# 5-fold cross-validation\nfolds = vfold_cv(peng_train, 5)\n\n# Get a rough estimate of the minimum training set size.\nn_train = floor((4 / 5) * 0.75 * nrow(peng_train))\n# Try k = 1, 3, 5, ..., n_train.\nks = seq(1, n_train, 2)\n\nform =\n  species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing \\(k\\)-fold cross-validation this way can produce inaccurate estimates when there are more observations for some classes than others (as is the case here).\nIt would be better to use stratified \\(k\\)-fold cross-validation, which attempts to preserve the overall class proportions within each fold. You can do this with the vfold_cv function by setting the strata parameter to the feature you want to use for stratification (in this case, we would use species).\n\n\nNow we can use nested apply functions or loops to iterate over the \\(k\\) and folds. For each fold, we‚Äôll get the training set and train the model with the current \\(k\\), then predict on the test set. We can use the predictions to compute performance statistics. To keep the example simple, let‚Äôs compute overall accuracy, although in practice it‚Äôs important to look at other statistics, including per-class statistics. The code to carry out these steps is:\n\n# Loop over k values.\nerror_rates = sapply(ks, function(k) {\n  # Loop over folds.\n  sapply(folds$splits, function(fold) {\n    # Fit the model on the training set.\n    train = training(fold)\n    model = nearest_neighbor(\"classification\", neighbors = k,\n      weight_func = \"rectangular\")\n    fitted = fit(model, form, train)\n\n    # Predict on the test set.\n    test = testing(fold)\n    preds = predict(fitted, test)\n    sum(test$species != preds$.pred_class) / nrow(test)\n  })\n})\n\nThe result from the code is a matrix of estimates. Each row corresponds to one value of \\(k\\), while each column corresponds to one fold. Let‚Äôs compute an overall estimate for each \\(k\\) with the colMeans function, and then plot the estimates against \\(k\\).\n\nestimates = data.frame(\n  k = ks,\n  error = colMeans(error_rates)\n)\n\nggplot(estimates) + aes(x = k, y = error) + geom_line()\n\n\n\n\n\n\n\n\nFor this particular data set, it looks like \\(k = 1\\) provides the best accuracy. This may seem somewhat surprising because of the bias-variance tradeoff, and it is a good idea to double-check your code in situations like this. However, for this data set, each penguin species is so well-separated by the four features that \\(k = 1\\) really does work well. If we wanted the use the model for new samples of penguins, it might still be a good idea to choose a slightly higher \\(k\\) value just in case this data set happens to have less variation than the population.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Supervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html",
    "href": "chapters/02_unsupervised.html",
    "title": "2¬† Unsupervised Learning",
    "section": "",
    "text": "2.1 Introduction\nUnsupervised learning methods identify or summarize groups or patterns within data. Unlike supervised learning, unsupervised learning methods do not require a training set and do not make predictions.\nThree common problems in unsupervised learning are:\nThe next section describes packages for unsupervised learning in R. The subsequent sections provide more details about and examples of methods for dimension reduction and clustering. The chapter ends with a discussion of ways to evaluate clusters.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html#introduction",
    "href": "chapters/02_unsupervised.html#introduction",
    "title": "2¬† Unsupervised Learning",
    "section": "",
    "text": "Dimension reduction, where the goal is to reduce the dimensionality (number of features) of a data set while preserving some of the characteristics that distinguish the observations. Some dimension reduction methods rank or select features, while others create an embedding, a set of new features computed from the originals.\nClustering, where the goal is to identify clusters, or groups of similar observations in a data set.\nAnomaly detection, where the goal is to identify unusual or extreme observations in a data set.\n\n\n\n\n\n\n\nTip\n\n\n\nIt might help to think of dimension reduction and clustering as unsupervised equivalents of regression and classification (Section 1.1), respectively. Dimension reduction creates a numerical summary (an embedding) of each observation, while clustering creates a categorical summary (a cluster label).\nYou can also think of dimension reduction and clustering methods as lossy data compression methods.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html#packages",
    "href": "chapters/02_unsupervised.html#packages",
    "title": "2¬† Unsupervised Learning",
    "section": "2.2 Packages",
    "text": "2.2 Packages\n\n2.2.1 Dimension Reduction\nThere‚Äôs no CRAN Task View page for dimension reduction. Some dimension reduction packages are:\n\nIndependent Component Analysis (ICA)\n\nfastICA\n\nPrincipal Component Analysis (PCA)\n\nBuilt-in prcomp (preferred) and princomp functions\nLearnPCA provides vignettes about PCA and details about the differences between prcomp and princomp\nkernlab for kernel PCA (as well as many other kernel-based machine learning methods)\n\nClassical/Metric Multidimensional Scaling (MDS)\n\nBuilt-in cmdscale function\nsmacof\n\nNonnegative Matrix Factorization (NMF)\n\nRcppML\nvegan\n\nt-Distributed Stochastic neighbor Embedding (t-SNE)\n\ntsne\n\nUniform Manifold Approximation and Projection (UMAP)\n\numap\nuwot\n\n\nThe following packages provide a wider collection of dimension reduction methods:\n\ndimRed is a common interface for a variety of dimension reduction methods packages (the accompanying paper provides an very brief overview of many dimension reduction methods).\nrecipes is a tidymodels package for preprocessing and feature engineering that provides a common interface for many dimension reduction methods packages.\nvegan is collection of statistical methods popular among community and vegetation ecologists, including some dimension reduction methods.\n\n\n\n2.2.2 Clustering\nThe CRAN Task View: Cluster Analysis page is a list of popular, actively-maintained packages for clustering. The list receives regular updates from the CRAN administrators.\nThe tidyclust package provides a common interface for a variety of clustering method packages.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html#dimension-reduction-1",
    "href": "chapters/02_unsupervised.html#dimension-reduction-1",
    "title": "2¬† Unsupervised Learning",
    "section": "2.3 Dimension Reduction",
    "text": "2.3 Dimension Reduction\nReasons you might want to use dimension reduction methods on a data set include:\n\nMany machine learning methods perform poorly when dimensionality is high, especially if it‚Äôs high relative to the number of observations.\nVisualizing more than 2 or 3 features at a time is difficult.\nYou suspect the data set has a few informative features hidden among many uninformative ones.\nComputing costs (time and money) tend to increase with data set size.\n\n\n\n\n\n\n\nImportant\n\n\n\nHigh-dimensional data sets are a problem for many machine learning methods because:\n\nSparsity increases with dimensionality, because volume increases with dimensionality. In other words, as the number of features in a data set increases, distances between the observations increase. This effect is known as the curse of dimensionality.\nThe number of parameters to estimate typically increases with dimensionality. For instance, in a linear model, there‚Äôs a coefficient for each included feature.\n\nA few methods, such as regularized linear models and random forest models, are robust by design when faced with high-dimensional data. These methods usually have some sort of built-in dimension reduction.\nSection 2.3.2 provides more details about the curse of dimensionality.\n\n\nDifferent dimension reduction methods preserve different characteristics of data and have different use cases: there‚Äôs no one-size-fits-all method. When selecting a method, think about what your goals and priorities are. Do you want to plot the data? Do you want to use the data for supervised learning? Do the data need to fit in a fixed amount of memory? Are the features numerical, categorical, or a mix of both? Thinking through these details will help guide you to an appropriate method. Nguyen and Holmes (2019) provide a concise overview of things to consider when using dimension reduction methods.\n\n\n\n\n\n\nNote\n\n\n\nIf you plan to use your data set for supervised learning, it‚Äôs usually best to choose a supervised learning model with some kind of dimension reduction built-in. These models optimize for predicting the response, whereas unsupervised dimension reduction methods do not. Some examples include:\n\nRegularized regression (LASSO, Elastic Net, ‚Ä¶)\nPartial least squares regression\nSupervised principal components analysis\nRandom forest models\n\n\n\nDimension reduction methods always change some of the relationships between observations. Typically, there‚Äôs a tradeoff between preserving global structure, the relationships between all observations, and local structure, the relationships between nearby observations. See this StackOverflow post for an example and visualization of this tradeoff.\nHere‚Äôs a list of a few well-known dimension reduction methods, where üåê denotes methods that tend to preserve global structure while üèòÔ∏è denotes methods that tend to preserve local structure:\n\nüåê Principal Component Analysis (PCA)\n\nIdentifies perpendicular axes of greatest variance (the ‚Äúprincipal components‚Äù) and rotates feature space to these axes\nDifficult to interpret in most cases\nCreates linear combinations of the original features; kernel PCA (kPCA) generalizes to non-linear combinations\n\nüåê Nonnegative Matrix Factorization (NMF)\n\nLike PCA, but limited to nonnegative data and produces nonnegative components\nEasy to interpret in some cases\n\nüèòÔ∏è Isomap\n\nAssumes observations are on a low-dimensional manifold and preserves distances on the manifold (geodesic distances)\n\nüèòÔ∏è t-Distributed Stochastic Neighbor Embedding (t-SNE) (guide)\n\nNon-deterministic method to preserve local structure\nMainly used for visualizations\n\nüèòÔ∏è Uniform Manifold Approximation and Projection (UMAP)\n\nLike t-SNE\nMainly used for visualizations, although the creators claim it is broadly applicable\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMost dimension reduction methods are sensitive to the scales of features, and some also require that features have mean 0. It‚Äôs usually a good idea to standardize each feature before dimension reduction, by subtracting the mean and dividing by the standard deviation. In R, you can use the scale function to do this.\n\n\n\n2.3.1 Example: Penguins PCA\nLet‚Äôs try out principal components analysis on the Palmer Penguins data set from Section 1.2.1. Start by loading the data, removing the observations with missing values, and selecting the numerical features:\n\nlibrary(\"palmerpenguins\")\n\npeng = na.omit(penguins)\n\n# Columns to transform with PCA.\ncols = c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\")\nx = peng[cols]\n\nLet‚Äôs run PCA with R‚Äôs built-in prcomp function first. The function has parameters center and scale. to control whether the features are centered and scaled first. PCA is intended for centered data, so you should always center the data (and this is the default). Whether you should scale the data depends on how different the scales of your features are, but it‚Äôs usually a good idea if you‚Äôre not sure. Scaling features makes them harder to interpret, but PCA is generally hard to interpret anyways. Run PCA:\n\npca = prcomp(x, center = TRUE, scale. = TRUE)\npca\n\nStandard deviations (1, .., p=4):\n[1] 1.6569115 0.8821095 0.6071594 0.3284579\n\nRotation (n x k) = (4 x 4):\n                         PC1         PC2        PC3        PC4\nbill_length_mm     0.4537532 -0.60019490 -0.6424951  0.1451695\nbill_depth_mm     -0.3990472 -0.79616951  0.4258004 -0.1599044\nflipper_length_mm  0.5768250 -0.00578817  0.2360952 -0.7819837\nbody_mass_g        0.5496747 -0.07646366  0.5917374  0.5846861\n\n\nYou can think of the standard deviations as a measure of information in along each principal component. The principal components are always sorted from highest to lowest standard deviation.\nEach principal component is a linear combination of the features. The columns of the rotation matrix show the coefficients for the features in these linear combinations.\nYou can use the predict function to compute the PCA embeddings for a data set. By default, prcomp computes these automatically for the original data set, so in this case you can get the embeddings with either pca$retx or predict. Let‚Äôs get the embeddings with predict and plot the first two principal components:\n\n# Rotate the features to the principal components.\nembeddings = predict(pca, x)\nembeddings = data.frame(species = peng$species, embeddings)\n\nlibrary(\"ggplot2\")\n\nggplot(embeddings) +\n  aes(x = PC1, y = PC2, color = species, shape = species) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe plot shows that the principal components separate the Gentoo penguins, but there‚Äôs still some overlap between the other species. PCA is not designed to separate classes (and is not even aware of the classes), so this result isn‚Äôt surprising. Instead, PCA tries to represent variation in the data set efficiently. For instance, the first and second principal components often capture a substantial percentage of the overall variation.\nIt‚Äôs common to make another plot, called a scree plot, to visualize how well principal components account for variation. Specifically, a scree plot shows the percentage of total variance accounted for by each principal component. The prcomp function returns standard deviations principal components, which you can square to compute variances. The code to make a scree plot is:\n\nscree = data.frame(pc = seq_along(pca$sdev), var = pca$sdev^2)\n# Compute percentage of variance explained.\nscree$pct_var = scree$var / sum(scree$var)\n\nggplot(scree) +\n  aes(x = pc, y = pct_var) +\n  labs(x = \"Principal Component\", y = \"Percentage of Variance Explained\") +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nThe scree plot shows that the first and second prinicpal component account for almost 90% (70% + 20%) of the variance in the four features. When you use PCA for dimension reduction, you can use a scree plot to decide how many principal components to keep. A common approach is to look for an ‚Äúelbow‚Äù in the plot where keeping additional components provides a relatively small increase in percentage of variance accounted for.\nNow let‚Äôs run PCA with the recipes package. The package is designed around the idea of writing ‚Äúrecipes‚Äù for data analysis, which can include preprocessing, feature engineering, modeling, and more. Recipes always begin with a call to the recipe function, which takes a training data set as an argument. Then you can add (via the pipe operator %&gt;%) any number of step_ functions, which correspond to individual steps in the analysis. For PCA, you can use step_normalize (to center and scale) and step_pca. Next, the prep function runs the recipe. Finally, the juice and bake functions extract the result of a recipe on the training set or a new data set, respectively. Putting all of these ideas together, the code is:\n\n# install.packages(\"recipes\")\nlibrary(\"recipes\")\n\nembeddings_tidy =\n  recipe(peng) %&gt;%\n  step_normalize(all_of(cols)) %&gt;%\n  step_pca(all_of(cols)) %&gt;%\n  prep() %&gt;%\n  juice()\n\nggplot(embeddings_tidy) +\n  aes(x = PC1, y = PC2, color = species, shape = species) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe plot shows the same result as before. As of writing, the recipes package doesn‚Äôt appear to provide a convenient way to get diagnostic information about PCA or make a scree plot.\n\n\n2.3.2 Example: Visualizing the Curse of Dimensionality\nAnother way to think about the curse of dimensionality is in terms of where most of the volume is in a \\(d\\)-dimensional ball. As \\(d\\) increases, the fraction of volume near the surface of the ball increases. In this example, we‚Äôll visualize the relationship between dimensionality \\(d\\) and the fraction of volume near the surface of a ball.\nIn general, the volume of a \\(d\\)-dimensional ball with radius \\(r\\) is:\n\\[\nV_d(r) = C_d \\cdot r^d\n\\]\nWhere \\(C_d\\) is a constant that depends on the dimensionality. For example, the volumes of a circle and a sphere are respectively:\n\\[\n\\begin{aligned}\nV_2(r) &= \\pi \\cdot r^2\n\\\\\nV_3(r) &= \\frac{4}{3} \\pi \\cdot r^3\n\\end{aligned}\n\\]\nFor a ball with radius \\(r = 1\\), the volume is just \\(V_d(1) = C_d\\). Now consider a ball with radius \\(r = 1 - \\epsilon\\). The volume is:\n\\[\nV_d(1 - \\epsilon) = C_d \\cdot (1 - \\epsilon)^d\n\\]\nSubtracting a \\((1 - \\epsilon)\\)-ball from a 1-ball gives a shell at the surface of the 1-ball with thickness \\(\\epsilon\\). You can also find the volume of this \\(\\epsilon\\)-shell by subtracting the respective volumes. So the fraction of the 1-ball‚Äôs volume made up of the \\(\\epsilon\\)-shell is:\n\\[\n\\frac{V_d(1) - V_d(1 - \\epsilon)}{V_d(1)}\n=\n\\frac{C_d - C_d \\cdot (1 - \\epsilon)^d}{C_d}\n=\n1 - (1 - \\epsilon)^d\n\\]\nYou can use R to plot this quantity for different values of \\(\\epsilon\\) and \\(d\\):\n\nlibrary(\"ggplot2\")\n\nggplot() +\n  # Set aspect ratio to 1.\n  coord_fixed() +\n  xlim(0, 1) +\n  labs(x = \"Shell Thickness\", y = \"Fraction of Volume\") +\n  scale_color_discrete(\"Dimensions\", breaks = c(\"2\", \"3\", \"10\", \"20\")) +\n  scale_linetype_discrete(\"Dimensions\", breaks = c(\"2\", \"3\", \"10\", \"20\")) +\n  # Functions to plot.\n  geom_function(\n    fun = \\(x) 1 - (1 - x)^2, aes(color = \"2\", linetype = \"2\")) +\n  geom_function(\n    fun = \\(x) 1 - (1 - x)^3, aes(color = \"3\", linetype = \"3\")) +\n  geom_function(\n    fun = \\(x) 1 - (1 - x)^10, aes(color = \"10\", linetype = \"10\")) +\n  geom_function(\n    fun = \\(x) 1 - (1 - x)^20, aes(color = \"20\", linetype = \"20\"))\n\n\n\n\n\n\n\n\nThe plot shows that as \\(d\\) increases, the fraction of volume near the surface of the 1-ball increases. For instance, at \\(d = 20\\), almost 100% of the volume is within 0.25 of the ball‚Äôs surface.\nThis example is based on one in Section 1.4 of Pattern Recognition and Machine Learning.\n\n\n\n\n\n\nTip\n\n\n\nSection 2.5 of The Elements of Statistical Learning provides a different perspective on the curse of dimensionality.\nTo learn even more about the counterintuitive properties of high-dimensional spaces, watch Thinking Outside the 10-Dimensional Box by 3Blue1Brown.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html#clustering-1",
    "href": "chapters/02_unsupervised.html#clustering-1",
    "title": "2¬† Unsupervised Learning",
    "section": "2.4 Clustering",
    "text": "2.4 Clustering\n\n\n\nResults from 11 different clustering methods across different 6 data sets. Clustered points are blue, orange, green, or pink, while unclustered/noise points are black. Visualization from the scikit-learn User Guide.\n\n\nClustering methods identify groups or clusters of similar observations within a data set. From an exploratory data analysis perspective, clusters provide insight into subpopulations, which might merit individual investigation or followup confirmatory work. Some clustering methods can also detect noise or outlying observations. From a classification perspective, clusters can sometimes be used as approximate classes, to speed up the process of creating an annotated training set.\nAs with dimension reduction methods, different clustering methods emphasize different characteristics of data. In particular, they measure similarity between observations in different ways. Here‚Äôs a list of some well-known families of clustering methods, where ‚ú® indicates a good starting point for many problems:\n\n‚ú® Density-based clustering\n\nThese methods estimate density of observations and treat contiguous high-density regions as clusters.\nTakeaways: Easy to explain the intuition, but many technical details. No assumptions about cluster size or shape. Can identify noise observations. Computational cost scales well.\nMethods:\n\nDBSCAN (demo)\nOPTICS\nHDBSCAN (guide)\n\n\n‚ú® Hierarchical clustering\n\nThese methods form a hierarchy of clusters by repeatedly combining or splitting initial clusters. Clusters are selected to combine or split based on how they affect dissimilarity, which is measured with a linkage function. A variety of linkage functions can be used, and the choice of linkage function has a substantial impact on the resulting clusters (see this page).\nTakeaways: The hierachy of clusters can be visualized with a dendogram, and can be ‚Äúcut‚Äù at various levels to produce coarser or finer clusters. Different linkage functions provide specific effects. Doesn‚Äôt identify noise observations. Computational cost scales well.\nMethods:\n\nAgglomerative clustering (guide)\nDivisive clustering\n\n\nModel-based clustering\n\nThese methods assume observations are generated from a specific probability distribution, estimate the parameters of the distribution, and predict the most likely cluster labels.\nTakeaways: A good choice if you have knowledge about the probability distribution of your data. Gaussian models, which are the most common, find ellipsoid clusters. The number of clusters needs to be specified in advance. Can be used for fuzzy clustering, since the result for each observation is a set of cluster probabilities rather than a label.\nMethods:\n\nMixture models, which can be fitted with the expectation-maximization algorithm or Bayesian methods\n\n\nPartitioning clustering\n\nThese methods partition a data set into evenly-sized clusters based on distances between observations and cluster centroids.\nTakeaways: Easy to explain. Clusters are evenly-sized. The number of clusters needs to be specified in advance, although there are ways to estimate the number of clusters. Doesn‚Äôt identify noise observations. Computational cost scales well.\nMethods:\n\n\\(k\\)-means (guide, demo)\n\\(k\\)-medoids, or partitioning around means (PAM)\nMini-Batch \\(k\\)-means\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLike dimension reduction methods, most clustering methods are sensitive to the scales of features. So you should generally standardize each feature before clustering.\n\n\n\n2.4.1 Example: Importance of Standardizing\nTo demonstrate the importance of standardizing features before clustering, let‚Äôs run \\(k\\)-means clustering on the Palmer Penguins data set before and after standardizing. We‚Äôll use the tidyclust package to do the clustering. The package is designed to have an interface similar to parsnip (see Section 1.2.1). Here‚Äôs the code to initialize \\(k\\)-means clustering and specify the features to consider:\n\n# install.packages(\"tidyclust\")\nlibrary(\"tidyclust\")\n\nkm = k_means(num_clusters = 3)\n\nform = ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g\n\nNow ‚Äúfit‚Äù the \\(k\\)-means clusters. Since we know there are 3 penguin species, let‚Äôs request 3 clusters:\n\nfitted = fit(km, form, peng)\nfitted\n\ntidyclust cluster object\n\nK-means clustering with 3 clusters of sizes 140, 113, 80\n\nCluster means:\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1       41.12214      17.94643          189.6286    3461.250\n3       44.24336      17.44779          201.5487    4310.619\n2       48.66250      15.39750          219.9875    5365.938\n\nClustering vector:\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  1   1   1   1   1   1   2   1   1   2   1   1   2   1   2   1   1   1   2   1 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n  1   1   1   1   2   1   2   1   2   1   2   2   1   1   2   1   2   1   2   1 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n  2   1   1   2   1   2   1   2   1   1   1   1   1   1   1   2   1   2   1   2 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n  1   2   1   2   1   2   1   2   1   2   1   2   1   2   1   2   1   2   1   1 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n  1   1   2   1   1   2   1   2   1   2   1   2   1   2   1   2   1   2   1   1 \n101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n  1   2   1   2   1   2   1   2   2   2   1   1   1   1   1   1   1   1   1   2 \n121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n  1   2   1   2   1   1   1   2   1   2   1   2   1   2   1   1   1   1   1   1 \n141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n  2   1   1   1   1   2   2   3   2   3   3   2   2   3   2   3   2   3   2   3 \n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n  2   3   2   3   2   3   3   3   2   3   3   3   3   2   3   3   2   3   3   3 \n181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n  3   3   3   2   3   2   3   2   2   3   3   2   3   3   3   3   3   2   3   3 \n201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n  3   2   3   2   3   2   3   2   3   2   3   3   2   3   2   3   3   3   2   3 \n221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n  2   3   2   3   2   3   2   3   2   3   2   3   3   3   3   3   2   3   3   3 \n241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n  3   3   2   3   3   3   3   3   3   2   3   2   3   3   3   2   3   2   3   3 \n261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n  3   3   3   3   3   1   2   1   1   1   2   1   1   2   1   1   1   1   2   1 \n281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n  2   1   1   1   2   1   1   1   1   1   2   1   1   1   2   1   2   1   2   1 \n301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n  2   1   2   1   2   2   1   1   1   1   2   1   2   1   1   1   2   1   2   1 \n321 322 323 324 325 326 327 328 329 330 331 332 333 \n  1   1   2   1   1   2   1   1   2   1   1   2   1 \n\nWithin cluster sum of squares by cluster:\n[1] 9724809 9318036 9718829\n (between_SS / total_SS =  86.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nPrinting the fitted object shows the cluster means as well as the ratio of the between-cluster sum of squared errors (BSS) to total sum of squared errors (TSS). The ratio provides one indication of how well the clusters fit the data; larger values are better.\nLet‚Äôs make a plot of the clusters against bill_length_mm and body_mass_g. You can get the cluster labels for the observations with the extract_cluster_assignment function. The result is a data frame with the cluster labels in the .cluster column. Here‚Äôs the code to make the plot:\n\nclust = extract_cluster_assignment(fitted)$.cluster\n\nggplot(peng) +\n  aes(x = bill_length_mm, y = body_mass_g, shape = clust, color = clust) +\n  geom_point() +\n  facet_wrap(vars(species))\n\n\n\n\n\n\n\n\nThe identified clusters don‚Äôt match up well with the penguin species. While this could just mean that the features don‚Äôt differentiate the species, in this case something else is at play. Notice that along the y-axis, the clusters fall into three distinct bands. This is a sign that body_mass_g, the feature on the y-axis, dominates the distances used to compute the clusters. In other words, these features have wildly different scales. You can see this by computing the standard deviations of the features:\n\nsummarize(peng, across(bill_length_mm:body_mass_g, sd))\n\n# A tibble: 1 √ó 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1           5.47          1.97              14.0        805.\n\n\nThe values of body_mass_g are much more spread out than the values of the other features.\n\npeng_scaled =\n  mutate(peng,\n    across(bill_length_mm:body_mass_g, \\(x) scale(x)[, 1])\n  )\n\nfitted = fit(km, form, peng_scaled)\nfitted\n\ntidyclust cluster object\n\nK-means clustering with 3 clusters of sizes 129, 85, 119\n\nCluster means:\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n3     -1.0452359     0.4858944        -0.8803701  -0.7616078\n2      0.6710153     0.8040534        -0.2889118  -0.3835267\n1      0.6537742    -1.1010497         1.1607163   1.0995561\n\nClustering vector:\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  1   1   1   1   1   1   1   1   1   1   1   1   2   1   2   1   1   1   1   1 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n  1   1   1   2   1   1   1   2   1   1   1   1   1   1   1   2   1   1   1   1 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n  1   1   1   2   1   1   1   2   1   2   1   1   1   2   1   2   1   1   1   1 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n  1   1   1   1   1   2   1   1   1   2   1   1   1   2   1   2   1   1   1   1 \n101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n  1   1   1   2   1   2   1   2   1   2   1   1   1   1   1   1   1   1   1   1 \n121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n  1   1   1   2   1   2   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n  1   1   1   1   1   2   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n  3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3   3 \n261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n  3   3   3   3   3   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 \n281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n  2   2   2   2   2   1   2   1   2   2   2   2   2   2   2   1   2   1   2   2 \n301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n  2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   1 \n321 322 323 324 325 326 327 328 329 330 331 332 333 \n  2   2   2   2   2   2   2   2   2   2   2   2   2 \n\nWithin cluster sum of squares by cluster:\n[1] 120.7030 109.4813 139.4684\n (between_SS / total_SS =  72.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nNotice that the BSS to TSS ratio decreased by about 14%.\nLet‚Äôs plot the clusters the same way as before:\n\nclust = extract_cluster_assignment(fitted)$.cluster\n\nggplot(peng) +\n  aes(x = bill_length_mm, y = body_mass_g, shape = clust, color = clust) +\n  geom_point() +\n  facet_wrap(vars(species))\n\n\n\n\n\n\n\n\nNow the clusters appear to correspond to the penguin species much better than before. Because all of the features have the same scale, they contribute equally to the distances \\(k\\)-means computes, and all of them help to identify the clusters.\nInterestingly, the BSS to TSS ratio is worse even though we know these clusters correspond better to the subpopulations in the data set. This demonstrates that there is no single best metric for cluster quality.\n\n\n2.4.2 Example: HDBSCAN\nHDBSCAN is an excellent starting point for clustering data if you suspect the clusters have irregular shapes, because it doesn‚Äôt make any shape assumptions. In this example, let‚Äôs run HDBSCAN on two different data sets.\nFirst, let‚Äôs try HDBSCAN on the Palmer Penguins data set, in order to compare the results to the example in Section 2.4.1. We‚Äôll use the dbscan package, since tidyclust doesn‚Äôt support HDBSCAN yet.\nHDBSCAN only has one required hyperparameter: the minimum cluster size. For the penguins data, let‚Äôs set this at 20.\n\n\n\n\n\n\nTip\n\n\n\nThis page provides some advice about how to choose the minimum cluster size.\n\n\nHere‚Äôs the code to standardize the features and run HDBSCAN:\n\n# install.packages(\"dbscan\")\nlibrary(\"dbscan\")\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nx = scale(peng[cols])\nfitted = hdbscan(x, 20)\nfitted\n\nHDBSCAN clustering for 333 objects.\nParameters: minPts = 20\nThe clustering contains 2 cluster(s) and 4 noise points.\n\n  0   1   2 \n  4 118 211 \n\nAvailable fields: cluster, minPts, coredist, cluster_scores,\n                  membership_prob, outlier_scores, hc\n\n\nThe cluster label 0 denotes noise. For this data set, HDBSCAN found 2 clusters, even though there are 3 species. One possible explanation is that the species have substantial overlap in some of the features. Density-based methods such as HDBSCAN have trouble with overlap if the density of observations remains high.\nLet‚Äôs plot the clusters:\n\nclust = factor(fitted$cluster)\n\nggplot(peng) +\n  aes(x = bill_length_mm, y = body_mass_g, shape = clust, color = clust) +\n  geom_point() +\n  facet_wrap(vars(species))\n\n\n\n\n\n\n\n\nBased on the plot, HDBSCAN clustered the Ad√©lie and Chinstrap penguins together.\nThe ‚ÄúH‚Äù in ‚ÄúHDBSCAN‚Äù stands for ‚Äúhierarchical,‚Äù because HDBSCAN actually computes a hierarchy of clusters. You can view this hierarchy as a dendogram by plotting the hc element of the fitted clusters with plot:\n\nplot(fitted$hc)\n\n\n\n\n\n\n\n\nFor this data, HDBSCAN doesn‚Äôt find many interesting intermediate clusters: it goes directly from individual observations as clusters to two relatively large clusters. Nevertheless, if you want to get the cluster assignments at any given height in the dendogram, you can use the cutree function to do so. Here‚Äôs how to get the cluster assignments at height 1.4:\n\ncutree(fitted$hc, h = 1.4)\n\n  [1] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[112] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3\n[149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3\n[186] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[223] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[260] 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also use the cutree function with results from R‚Äôs built-in hierarchical clustering functions.\n\n\nHDBSCAN is worse at clustering the penguin species than \\(k\\)-means. Now let‚Äôs consider another data set where HDBSCAN is better at clustering than \\(k\\)-means. The DS3 data set, which is included in the dbscan package, consists of 8,000 observations arranged into 6 visually-distinct shapes and some noise. Here‚Äôs what the data set looks like:\n\ndata(\"DS3\")\n\nggplot(DS3) +\n  aes(x = X, y = Y) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nfitted = hdbscan(DS3, 20)\n\nclust = factor(fitted$cluster)\n\nggplot(DS3) +\n  aes(x = X, y = Y, color = clust) +\n  geom_point()\n\n\n\n\n\n\n\n\nFor the DS3 data, HDBSCAN succesfully clusters each of the shapes. Try running \\(k\\)-means clustering on the DS3 data with \\(k = 6\\) to see how well it does!",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/02_unsupervised.html#evaluating-clusters",
    "href": "chapters/02_unsupervised.html#evaluating-clusters",
    "title": "2¬† Unsupervised Learning",
    "section": "2.5 Evaluating Clusters",
    "text": "2.5 Evaluating Clusters\nJust as different clustering methods use different measures of (dis)similarity, there are many different ways to evaluate whether clusters are ‚Äúgood‚Äù. Two of these are:\n\nRatios of sums of squared error (as discussed in Section 2.4.1)\nSilhouette scores\n\nThis paper provides a brief discussion of ways to evaluate clusters, while the book it‚Äôs from provides more details. The scikit-learn User Guide also has a section that describes many different evaluation metrics. The best way to evaluate cluster assignments is often to carry out exploratory data analysis to investigate whether they match what you know about (and see in) the data.",
    "crumbs": [
      "**Machine Learning in R**",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Unsupervised Learning</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Tidy Modeling with R by M. Kuhn & J. Silge\nFeature Engineering and Selection: A Practical Approach for Predictive Models by M. Kuhn & K. Johnson\nAn Introduction to Statistical Learning by G. James et al.\nThe Elements of Statistical Learning: Data Mining, Inference, and Prediction by T. Hastie et al.\nHandbook of Cluster Analysis by C. Hennig et al.",
    "crumbs": [
      "References"
    ]
  }
]